14:36:08,88 root INFO Start training JAX-CanVeg with the configuration file configs.json under /global/cfs/cdirs/m1800/peishi/JAX-CanVeg/examples/US-Bi1/PB-1L-0.8
14:36:08,89 root INFO Loading training forcings and fluxes...
14:36:08,896 jax._src.xla_bridge INFO Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
14:36:08,914 jax._src.xla_bridge INFO Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
14:36:10,445 root INFO Loading test forcings and fluxes if any...
14:36:10,963 root INFO number of lagrangian particles is not found in configuration of model and return 1000000.
14:36:10,963 root INFO Loading the model setup and parameters ...
14:36:13,685 root INFO Loading the disperion matrix ...
14:36:13,686 root INFO Reading dispersion matrix from ../../../data/dij/Dij_US-Bi1_1L.csv
14:36:13,713 root INFO Getting model type CanvegIFT ...
14:36:13,715 root INFO Getting output function that gives canopy latent heat fluxes and net ecosystem exchange ...
14:36:14,82 root INFO Converting the obs and met to batched dataset ...
14:36:14,468 root INFO Getting the filtered model spec for the tunable parameters ...
14:36:14,470 root INFO Getting the loss function ...
14:36:14,483 root INFO Getting the optimizer and training epochs ...
14:37:21,483 root INFO The training loss of step 0: 0.18413131622724566; the test loss of step 0: 0.18719670480634606.
14:38:16,12 root INFO The training loss of step 1: 0.17579320527385653; the test loss of step 1: 0.18397493837383383.
14:38:32,415 root INFO The training loss of step 2: 0.17751963490095957; the test loss of step 2: 0.18453927927679378.
14:38:48,782 root INFO The training loss of step 3: 0.17350703303789738; the test loss of step 3: 0.18827426751917173.
14:39:05,222 root INFO The training loss of step 4: 0.16947624435468042; the test loss of step 4: 0.19467651762426622.
14:39:21,556 root INFO The training loss of step 5: 0.16871555299164315; the test loss of step 5: 0.20063938338390816.
14:39:37,954 root INFO The training loss of step 6: 0.1690113455683089; the test loss of step 6: 0.20301714149908281.
14:39:54,310 root INFO The training loss of step 7: 0.16911101792579492; the test loss of step 7: 0.2012594342813333.
14:40:10,735 root INFO The training loss of step 8: 0.16801393296199166; the test loss of step 8: 0.19710315021633573.
14:40:27,85 root INFO The training loss of step 9: 0.16695572819279192; the test loss of step 9: 0.19257063698652094.
14:40:43,604 root INFO The training loss of step 10: 0.16623846932972647; the test loss of step 10: 0.18890228472774054.
14:40:59,996 root INFO The training loss of step 11: 0.16561526840061466; the test loss of step 11: 0.186811215870481.
14:41:16,500 root INFO The training loss of step 12: 0.165505366596723; the test loss of step 12: 0.18541849240864655.
14:41:32,913 root INFO The training loss of step 13: 0.16513776785741127; the test loss of step 13: 0.1853289858600628.
14:41:49,356 root INFO The training loss of step 14: 0.1646091486742753; the test loss of step 14: 0.18647120639876685.
14:42:05,785 root INFO The training loss of step 15: 0.16397115946414528; the test loss of step 15: 0.18831045035360883.
14:42:22,194 root INFO The training loss of step 16: 0.16319511527871514; the test loss of step 16: 0.19106536476102412.
14:42:38,526 root INFO The training loss of step 17: 0.16330274069177145; the test loss of step 17: 0.19327316597700708.
14:42:54,959 root INFO The training loss of step 18: 0.1640898635716794; the test loss of step 18: 0.19474626482716342.
14:43:11,358 root INFO The training loss of step 19: 0.16385842680617974; the test loss of step 19: 0.19472227123609578.
14:43:27,790 root INFO The training loss of step 20: 0.16373755461799339; the test loss of step 20: 0.1935901158574116.
14:43:44,192 root INFO The training loss of step 21: 0.16351336567402097; the test loss of step 21: 0.19106366023295157.
14:44:00,626 root INFO The training loss of step 22: 0.1626353236436191; the test loss of step 22: 0.18839702088130594.
14:44:17,33 root INFO The training loss of step 23: 0.16194970042914628; the test loss of step 23: 0.1860243010210331.
14:44:33,454 root INFO The training loss of step 24: 0.1619203863747078; the test loss of step 24: 0.18425753290569524.
14:44:49,866 root INFO The training loss of step 25: 0.1616150864723694; the test loss of step 25: 0.18343469461107934.
14:45:06,306 root INFO The training loss of step 26: 0.1611694391207005; the test loss of step 26: 0.1829950620798399.
14:45:22,711 root INFO The training loss of step 27: 0.1611356395988179; the test loss of step 27: 0.18354387960578972.
14:45:39,142 root INFO The training loss of step 28: 0.16100557653009495; the test loss of step 28: 0.18453047708752984.
14:45:55,576 root INFO The training loss of step 29: 0.1611327735218716; the test loss of step 29: 0.18521266761270322.
14:46:12,9 root INFO The training loss of step 30: 0.16133674512229965; the test loss of step 30: 0.18566043253872438.
14:46:28,402 root INFO The training loss of step 31: 0.1614189018688469; the test loss of step 31: 0.18595732967334289.
14:46:44,862 root INFO The training loss of step 32: 0.16166667051744588; the test loss of step 32: 0.18516277660680402.
14:47:01,265 root INFO The training loss of step 33: 0.1615021939332822; the test loss of step 33: 0.18262852036904945.
14:47:17,712 root INFO The training loss of step 34: 0.16153756005012643; the test loss of step 34: 0.1801190242618028.
14:47:34,94 root INFO The training loss of step 35: 0.16133890490086256; the test loss of step 35: 0.17825925195375272.
14:47:50,582 root INFO The training loss of step 36: 0.1622015126785089; the test loss of step 36: 0.17751525420063066.
14:48:06,958 root INFO The training loss of step 37: 0.16206051251602882; the test loss of step 37: 0.17746785938935514.
14:48:23,387 root INFO The training loss of step 38: 0.1625914473468713; the test loss of step 38: 0.17786806807067096.
14:48:39,765 root INFO The training loss of step 39: 0.16294592563910104; the test loss of step 39: 0.17876208813516323.
14:48:56,193 root INFO The training loss of step 40: 0.16354844597862658; the test loss of step 40: 0.17985012813923684.
14:49:12,559 root INFO The training loss of step 41: 0.1638950366330946; the test loss of step 41: 0.18062807649741158.
14:49:28,985 root INFO The training loss of step 42: 0.16431132158307704; the test loss of step 42: 0.18092748394223326.
14:49:45,396 root INFO The training loss of step 43: 0.16556050677138398; the test loss of step 43: 0.18060662139123065.
14:50:01,825 root INFO The training loss of step 44: 0.16492788892893107; the test loss of step 44: 0.1800766310594623.
14:50:18,292 root INFO The training loss of step 45: 0.16591592695865376; the test loss of step 45: 0.17989846321462105.
14:50:34,702 root INFO The training loss of step 46: 0.16554948672469047; the test loss of step 46: 0.179871824229675.
14:50:51,48 root INFO The training loss of step 47: 0.167361909103296; the test loss of step 47: 0.18000503826531444.
14:51:07,418 root INFO The training loss of step 48: 0.1678088964204041; the test loss of step 48: 0.18080622139828445.
14:51:23,820 root INFO The training loss of step 49: 0.16855823267804262; the test loss of step 49: 0.18224911024156792.
14:51:40,217 root INFO The training loss of step 50: 0.1694336465759723; the test loss of step 50: 0.18293246325537263.
14:51:56,626 root INFO The training loss of step 51: 0.17050334600652223; the test loss of step 51: 0.18351399519389894.
14:52:13,12 root INFO The training loss of step 52: 0.1715817500170117; the test loss of step 52: 0.18432951879553777.
14:52:29,418 root INFO The training loss of step 53: 0.17264562793021254; the test loss of step 53: 0.18502975141830325.
14:52:45,794 root INFO The training loss of step 54: 0.17228495139717945; the test loss of step 54: 0.18555589474258996.
14:53:02,204 root INFO The training loss of step 55: 0.1732629831019632; the test loss of step 55: 0.18535498801693223.
14:53:18,594 root INFO The training loss of step 56: 0.17421168989173; the test loss of step 56: 0.18539549138436473.
14:53:34,997 root INFO The training loss of step 57: 0.17510989605492883; the test loss of step 57: 0.18582232910761146.
14:53:51,364 root INFO The training loss of step 58: 0.176170620002977; the test loss of step 58: 0.18571567316231655.
14:54:07,780 root INFO The training loss of step 59: 0.17735817233014248; the test loss of step 59: 0.18557804686353666.
14:54:24,151 root INFO The training loss of step 60: 0.1773604728545828; the test loss of step 60: 0.18590353597553946.
14:54:40,553 root INFO The training loss of step 61: 0.17812510003964205; the test loss of step 61: 0.1859094719353669.
14:54:56,923 root INFO The training loss of step 62: 0.17861754831427568; the test loss of step 62: 0.18567735968424937.
14:55:13,346 root INFO The training loss of step 63: 0.1801430165358451; the test loss of step 63: 0.1860166083829459.
14:55:29,875 root INFO The training loss of step 64: 0.18110282763486166; the test loss of step 64: 0.1860187875144799.
14:55:46,295 root INFO The training loss of step 65: 0.18209322666315816; the test loss of step 65: 0.18606013140956856.
14:56:02,699 root INFO The training loss of step 66: 0.1820380736608193; the test loss of step 66: 0.18596517313770922.
14:56:19,122 root INFO The training loss of step 67: 0.18324608210058405; the test loss of step 67: 0.18608694187042343.
14:56:35,506 root INFO The training loss of step 68: 0.184852929997058; the test loss of step 68: 0.18627402483515768.
14:56:51,919 root INFO The training loss of step 69: 0.18478274168047032; the test loss of step 69: 0.186159087101201.
14:57:08,327 root INFO The training loss of step 70: 0.18658233243740438; the test loss of step 70: 0.18620349759630359.
14:57:24,742 root INFO The training loss of step 71: 0.18734614848521408; the test loss of step 71: 0.18591931502903006.
14:57:41,124 root INFO The training loss of step 72: 0.18908726576690776; the test loss of step 72: 0.18614286835686664.
14:57:57,543 root INFO The training loss of step 73: 0.1887881937663986; the test loss of step 73: 0.18610617528952272.
14:58:13,950 root INFO The training loss of step 74: 0.19018975845585273; the test loss of step 74: 0.18623651315716105.
14:58:30,382 root INFO The training loss of step 75: 0.1909758861632278; the test loss of step 75: 0.18653638910703357.
14:58:46,783 root INFO The training loss of step 76: 0.19206825115548148; the test loss of step 76: 0.18729225559177282.
14:59:03,228 root INFO The training loss of step 77: 0.19277248121116533; the test loss of step 77: 0.18790985715488806.
14:59:19,636 root INFO The training loss of step 78: 0.19440311481356112; the test loss of step 78: 0.18767279450045243.
14:59:36,77 root INFO The training loss of step 79: 0.19747594173830516; the test loss of step 79: 0.18800951831721985.
14:59:52,482 root INFO The training loss of step 80: 0.199660464138617; the test loss of step 80: 0.18874565935747148.
15:00:08,908 root INFO The training loss of step 81: 0.19927829792811047; the test loss of step 81: 0.18917829113916168.
15:00:25,313 root INFO The training loss of step 82: 0.20049507656743656; the test loss of step 82: 0.1898412384499022.
15:00:41,752 root INFO The training loss of step 83: 0.20476171021180972; the test loss of step 83: 0.19009903564852204.
15:00:58,147 root INFO The training loss of step 84: 0.20685805470506627; the test loss of step 84: 0.1903345539502597.
15:01:14,599 root INFO The training loss of step 85: 0.20738723900704967; the test loss of step 85: 0.1903845130137911.
15:01:30,994 root INFO The training loss of step 86: 0.2094208117424935; the test loss of step 86: 0.19097206164527397.
15:01:47,412 root INFO The training loss of step 87: 0.20818365511702724; the test loss of step 87: 0.19178508384192847.
15:02:03,807 root INFO The training loss of step 88: 0.208582122034777; the test loss of step 88: 0.19189686449104063.
15:02:20,320 root INFO The training loss of step 89: 0.2098834294847758; the test loss of step 89: 0.19190361096136152.
15:02:36,713 root INFO The training loss of step 90: 0.2109751752466338; the test loss of step 90: 0.1927785915983444.
15:02:53,138 root INFO The training loss of step 91: 0.21360182455180537; the test loss of step 91: 0.19264976170847536.
15:03:09,543 root INFO The training loss of step 92: 0.2129348129990372; the test loss of step 92: 0.1927871578679789.
15:03:25,984 root INFO The training loss of step 93: 0.21474720949399953; the test loss of step 93: 0.1935011146110713.
15:03:42,389 root INFO The training loss of step 94: 0.21342368747576146; the test loss of step 94: 0.19333705158173228.
15:03:58,820 root INFO The training loss of step 95: 0.21986834849663378; the test loss of step 95: 0.19339510679799043.
15:04:15,236 root INFO The training loss of step 96: 0.2219757177487697; the test loss of step 96: 0.193615231369025.
15:04:31,681 root INFO The training loss of step 97: 0.22257156750324777; the test loss of step 97: 0.19362673174824496.
15:04:48,113 root INFO The training loss of step 98: 0.2264163304587799; the test loss of step 98: 0.19364460657506646.
15:05:04,563 root INFO The training loss of step 99: 0.22990412065463214; the test loss of step 99: 0.19452585534072842.
15:05:20,959 root INFO The training loss of step 100: 0.2334502214723738; the test loss of step 100: 0.19439292539892297.
15:05:37,390 root INFO The training loss of step 101: 0.23439594758587232; the test loss of step 101: 0.1945270233960539.
15:05:53,802 root INFO The training loss of step 102: 0.23287799918495716; the test loss of step 102: 0.19525256275365288.
15:06:10,251 root INFO The training loss of step 103: 0.23027451333987004; the test loss of step 103: 0.1953456185718176.
15:06:26,661 root INFO The training loss of step 104: 0.2332770876677581; the test loss of step 104: 0.19606360299618994.
15:06:43,95 root INFO The training loss of step 105: 0.23395194299915817; the test loss of step 105: 0.19581322934647014.
15:06:59,496 root INFO The training loss of step 106: 0.23780459085954456; the test loss of step 106: 0.19612227279768024.
15:07:15,932 root INFO The training loss of step 107: 0.2384529656708408; the test loss of step 107: 0.19669090029485076.
15:07:32,321 root INFO The training loss of step 108: 0.23735506585913074; the test loss of step 108: 0.1969558336333598.
15:07:48,746 root INFO The training loss of step 109: 0.2364415760735099; the test loss of step 109: 0.19702299762726502.
15:08:05,192 root INFO The training loss of step 110: 0.2408521054526111; the test loss of step 110: 0.1964267208273742.
15:08:21,611 root INFO The training loss of step 111: 0.24428377728241843; the test loss of step 111: 0.1964056630157143.
15:08:38,2 root INFO The training loss of step 112: 0.2437641460217782; the test loss of step 112: 0.19680595636260817.
15:08:54,435 root INFO The training loss of step 113: 0.24322859188526919; the test loss of step 113: 0.1961252576193269.
15:09:10,842 root INFO The training loss of step 114: 0.24300011967394072; the test loss of step 114: 0.19596207859847872.
15:09:27,272 root INFO The training loss of step 115: 0.24621338992517175; the test loss of step 115: 0.19662954890624168.
15:09:43,658 root INFO The training loss of step 116: 0.24661332569272562; the test loss of step 116: 0.1965297449290569.
15:10:00,90 root INFO The training loss of step 117: 0.2471592244214298; the test loss of step 117: 0.19606826316566056.
15:10:16,501 root INFO The training loss of step 118: 0.24437127551411747; the test loss of step 118: 0.19731041262417195.
15:10:32,939 root INFO The training loss of step 119: 0.24506954431213307; the test loss of step 119: 0.19702992280673975.
15:10:49,325 root INFO The training loss of step 120: 0.24820101347417226; the test loss of step 120: 0.19712811201489494.
15:11:05,710 root INFO The training loss of step 121: 0.24812173626016734; the test loss of step 121: 0.1972310677969577.
15:11:22,127 root INFO The training loss of step 122: 0.24726572650535567; the test loss of step 122: 0.19707831419485122.
15:11:38,548 root INFO The training loss of step 123: 0.24668723864269773; the test loss of step 123: 0.19734368014312914.
15:11:54,926 root INFO The training loss of step 124: 0.24630279671864236; the test loss of step 124: 0.19725520248683448.
15:12:11,320 root INFO The training loss of step 125: 0.25052458955261364; the test loss of step 125: 0.19647023388668608.
15:12:27,748 root INFO The training loss of step 126: 0.2520276608582988; the test loss of step 126: 0.1967286484099894.
15:12:44,180 root INFO The training loss of step 127: 0.25421487867564135; the test loss of step 127: 0.1967735134020177.
15:13:00,647 root INFO The training loss of step 128: 0.2530847848840903; the test loss of step 128: 0.19755494264299225.
15:13:17,85 root INFO The training loss of step 129: 0.2581069477587076; the test loss of step 129: 0.1979052679238272.
15:13:33,493 root INFO The training loss of step 130: 0.25660329707001933; the test loss of step 130: 0.1977836574750342.
15:13:49,869 root INFO The training loss of step 131: 0.2606016850959879; the test loss of step 131: 0.19673551826226698.
15:14:06,270 root INFO The training loss of step 132: 0.2639861412606986; the test loss of step 132: 0.19741422767778102.
15:14:22,740 root INFO The training loss of step 133: 0.2608163883192741; the test loss of step 133: 0.19712216663883741.
15:14:39,132 root INFO The training loss of step 134: 0.26304775247626294; the test loss of step 134: 0.19721699171124382.
15:14:55,644 root INFO The training loss of step 135: 0.26147327156373923; the test loss of step 135: 0.19711348940125714.
15:15:12,86 root INFO The training loss of step 136: 0.2637229298978096; the test loss of step 136: 0.1975079060623738.
15:15:28,514 root INFO The training loss of step 137: 0.26307081225733514; the test loss of step 137: 0.1981430051147067.
15:15:44,985 root INFO The training loss of step 138: 0.2640601480257358; the test loss of step 138: 0.1986986237865783.
15:16:01,413 root INFO The training loss of step 139: 0.2647075923763399; the test loss of step 139: 0.19866166883160524.
15:16:17,779 root INFO The training loss of step 140: 0.2697240126721823; the test loss of step 140: 0.19901629879544275.
15:16:34,167 root INFO The training loss of step 141: 0.2724286585088162; the test loss of step 141: 0.20029922080519422.
15:16:50,611 root INFO The training loss of step 142: 0.27345138978246947; the test loss of step 142: 0.20027581505163103.
15:17:06,964 root INFO The training loss of step 143: 0.2753868173304177; the test loss of step 143: 0.20070350500318307.
15:17:23,349 root INFO The training loss of step 144: 0.2695775476527825; the test loss of step 144: 0.20095635435544068.
15:17:39,769 root INFO The training loss of step 145: 0.27429438562484176; the test loss of step 145: 0.2008131650151361.
15:17:56,288 root INFO The training loss of step 146: 0.277616500660021; the test loss of step 146: 0.20107267001564466.
15:18:12,695 root INFO The training loss of step 147: 0.2837018159726739; the test loss of step 147: 0.20003426308552882.
15:18:29,111 root INFO The training loss of step 148: 0.2875238807795087; the test loss of step 148: 0.2004830418149312.
15:18:45,552 root INFO The training loss of step 149: 0.2878976803539677; the test loss of step 149: 0.2004864217881308.
15:19:02,163 root INFO The training loss of step 150: 0.2936903029751499; the test loss of step 150: 0.2008146532343441.
15:19:18,592 root INFO The training loss of step 151: 0.2885726682607898; the test loss of step 151: 0.20046360342359518.
15:19:35,53 root INFO The training loss of step 152: 0.2872613729705012; the test loss of step 152: 0.20100138374702567.
15:19:51,591 root INFO The training loss of step 153: 0.28593702137563615; the test loss of step 153: 0.20054632601820113.
15:20:07,972 root INFO The training loss of step 154: 0.28666916624639216; the test loss of step 154: 0.20129156107155602.
15:20:24,457 root INFO The training loss of step 155: 0.30151493990791145; the test loss of step 155: 0.20101153046241427.
15:20:40,904 root INFO The training loss of step 156: 0.2952952095661662; the test loss of step 156: 0.20183681289036698.
15:20:57,318 root INFO The training loss of step 157: 0.29250361351963305; the test loss of step 157: 0.2020334037074057.
15:21:13,778 root INFO The training loss of step 158: 0.29384597891410524; the test loss of step 158: 0.20226774295331204.
15:21:30,207 root INFO The training loss of step 159: 0.2872361262726078; the test loss of step 159: 0.2024540614509023.
15:21:46,592 root INFO The training loss of step 160: 0.2952688512540264; the test loss of step 160: 0.2026406365797219.
15:22:03,57 root INFO The training loss of step 161: 0.29716949534799897; the test loss of step 161: 0.20266649419877064.
15:22:19,484 root INFO The training loss of step 162: 0.2954296395978164; the test loss of step 162: 0.2023847907731938.
15:22:35,901 root INFO The training loss of step 163: 0.30069671600990405; the test loss of step 163: 0.20190899902007486.
15:22:52,356 root INFO The training loss of step 164: 0.30556631130742035; the test loss of step 164: 0.20199112393797305.
15:23:08,767 root INFO The training loss of step 165: 0.3259910604763828; the test loss of step 165: 0.2022781631982092.
15:23:25,201 root INFO The training loss of step 166: 0.33234456232691445; the test loss of step 166: 0.20171445059333415.
15:23:41,657 root INFO The training loss of step 167: 0.335479294098453; the test loss of step 167: 0.20317769629030877.
15:23:58,48 root INFO The training loss of step 168: 0.33057968171435653; the test loss of step 168: 0.2020581550879341.
15:24:14,496 root INFO The training loss of step 169: 0.32973417206920025; the test loss of step 169: 0.20247111573234486.
15:24:30,931 root INFO The training loss of step 170: 0.3259819589271338; the test loss of step 170: 0.20247532784749983.
15:24:47,306 root INFO The training loss of step 171: 0.32298105902392643; the test loss of step 171: 0.20216801187775837.
15:25:03,694 root INFO The training loss of step 172: 0.3207128779756479; the test loss of step 172: 0.2035560177667993.
15:25:19,663 root WARNING The current loss value 952.8167212780639 is way larger than the previous ones. Stop training.
15:25:19,664 root INFO Saving the loss values ...
15:25:19,901 root INFO Saving the trained model ...
