{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import timeit\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import Tuple, List\n",
    "from jaxtyping import Float, Array\n",
    "import equinox as eqx\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import jax\n",
    "from jax import jit, vmap, lax, jacfwd, jacrev, grad, vjp, jvp, random\n",
    "import jax.numpy as jnp\n",
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update(\"jax_debug_infs\", True)\n",
    "config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "This notebook explains how to implement backpropagation through implicit layers. As an example problem, we consider the following problem:\n",
    "$$\n",
    "\\begin{align}\n",
    "% \\mathbf{F}(\\mathbf{x}, \\alpha, \\beta) = \\mathbf{0},\n",
    "\\mathbf{F}(\\mathbf{x}, \\alpha, \\beta) = \\mathbf{x},\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{x} = (x_1, x_2)$ and\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{F}(\\mathbf{x}, \\alpha, \\beta) = \\left[\\begin{matrix}\n",
    "% x_1^2 + x_2^2 - \\alpha \\\\\n",
    "% \\beta x_1^3 - x_2  \n",
    "% x_1^2 + x_2^2 + x_1 - \\alpha \\\\\n",
    "% \\beta x_1^3\n",
    "\\sqrt{\\alpha-x_1x_2} \\\\\n",
    "\\sqrt{\\frac{57-x_2}{\\beta x_1}}\n",
    "\\end{matrix}\\right]\n",
    "\\end{align}\n",
    "$$. When $\\alpha = 4, \\beta = 1$, the approximate real solution is $(x_1, x_2) = (\\pm 1.174, \\pm 1.619)$, which is clear in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Para(eqx.Module):\n",
    "    alpha: Float[Array, \"\"]\n",
    "    beta: Float[Array, \"\"]\n",
    "\n",
    "\n",
    "class Fc(eqx.Module):\n",
    "    para: Para\n",
    "\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        F_0 = jnp.sqrt(self.para.alpha - x[0] * x[1])\n",
    "        F_1 = jnp.sqrt((57 - x[1]) / (self.para.beta * F_0))\n",
    "        # return F_0, F_1\n",
    "        return jnp.array([F_0, F_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2.17944947, 2.86050599], dtype=float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_v = 10.0\n",
    "beta_v = 3.0\n",
    "\n",
    "para = Para(alpha_v, beta_v)\n",
    "\n",
    "func = Fc(para)\n",
    "\n",
    "func(jnp.array([1.5, 3.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed-point method to solve the system of non-linear equations\n",
    "$$\n",
    "\\begin{align}\n",
    "% \\mathbf{F}(\\mathbf{x}, \\alpha, \\beta) = \\mathbf{0},\n",
    "\\mathbf{x_{n+1}} = \\mathbf{F}(\\mathbf{x_n}, \\alpha, \\beta),\n",
    "\\end{align}\n",
    "$$\n",
    "where $n$ is the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_point(f, x_guess):\n",
    "    def body_fun(x, i):\n",
    "        x_new = f(x)\n",
    "        # jax.debug.print(\"x_new: {a}\", a=x_new)\n",
    "        return x_new, x_new - x\n",
    "\n",
    "    x_star, _ = jax.lax.scan(body_fun, x_guess, xs=None, length=1000)\n",
    "    return x_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2., 3.], dtype=float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_guess = jnp.array([1.0, 1.0])\n",
    "x_guess = jnp.array([1.5, 3.5])\n",
    "fixed_point(func, x_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the gradient through the fixed-point iteration solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_guess = jnp.array([1.5, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2., 3.], dtype=float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def implicit_func_naive(func, x_guess):\n",
    "    x_solution = fixed_point(func, x_guess)\n",
    "    return x_solution\n",
    "\n",
    "\n",
    "implicit_func_naive(func, x_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_implicit_naive = jacfwd(implicit_func_naive, argnums=(0,), has_aux=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 0.1804878 , -0.13170732], dtype=float64),\n",
       " Array([ 0.17560976, -0.61463415], dtype=float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(jac,) = derivative_implicit_naive(func, x_guess)\n",
    "jac.para.alpha, jac.para.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiating through implicit funciton through implicit funciton theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement custom derivative rule for the Newton solver so that we can use jax.grad or jax.jacrev through the Newton solver.\n",
    "$$\n",
    "\\begin{align}\n",
    "F(\\mathbf{x}, \\mathbf{W}) = \\mathbf{x},\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{W} = (\\alpha, \\beta)$. Because both sides are the same functions, their derivatives are the same. Using the chain rule on the left hand side, \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial F}{\\partial \\mathbf{x}} \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{W}} + \\frac{\\partial F}{\\partial \\mathbf{W}} = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{W}}\n",
    "\\end{align}\n",
    "$$\n",
    "Thus,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{W}} = \\left[\\mathbf{I}-\\frac{\\partial F}{\\partial \\mathbf{x}}\\right]^{-1}\\frac{\\partial F}{\\partial \\mathbf{W}}\n",
    "\\end{align}\n",
    "$$\n",
    "The Jacobian vector product (w is the vector to be multipled) is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{W}}w = \\left[\\mathbf{I}-\\frac{\\partial F}{\\partial \\mathbf{x}}\\right]^{-1}u,\n",
    "\\end{align}\n",
    "$$\n",
    "where $u = \\frac{\\partial F}{\\partial \\mathbf{W}}w$ can be computed by Jacobian vector product of $\\mathbf{F}$. Then, the Jacobian vector product $\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{W}}w$ is the solution to the linear system \n",
    "$$\n",
    "\\left[\\mathbf{I}-\\frac{\\partial F}{\\partial \\mathbf{x}}\\right] \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{W}}w = u.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the implicit differentiation in JAX by Jacobian-vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.custom_jvp, nondiff_argnums=(0,))\n",
    "def implicit_func(func, x_guess, para):\n",
    "    func = eqx.tree_at(lambda t: t.para, func, para)\n",
    "    x_solution = fixed_point(func, x_guess)\n",
    "    return x_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@implicit_func.defjvp\n",
    "def implicit_func_jvp(func, primals, tangents):\n",
    "    def func_para(func, x, para):\n",
    "        func = eqx.tree_at(lambda t: t.para, func, para)\n",
    "        return func(x)\n",
    "\n",
    "    x_guess, args = primals[0], primals[1:]\n",
    "    tangents_x, tangents_args = tangents[0], tangents[1:]\n",
    "\n",
    "    para = args[0]\n",
    "    x_solution = implicit_func(func, x_guess, para)\n",
    "\n",
    "    _, u = jvp(partial(func_para, func, x_solution), args, tangents_args, has_aux=False)\n",
    "    # _, u = jvp(partial(func_para, func, x_solution), args, args, has_aux=False)\n",
    "    Jacobian_JAX = jacfwd(func, argnums=0, has_aux=False)\n",
    "    J = Jacobian_JAX(x_solution)\n",
    "    # J, u = jnp.array(J), jnp.array(u)\n",
    "    I = jnp.eye(J.shape[0])\n",
    "    tangent_out = jnp.linalg.solve(I - J, u)\n",
    "    return (\n",
    "        x_solution,\n",
    "        tangent_out,\n",
    "    )  # you don't need to add None, see the discussion here (https://github.com/google/jax/discussions/16871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0., 0.],\n",
       "        [0., 0.]], dtype=float64),\n",
       " Array([ 0.1804878 , -0.13170732], dtype=float64, weak_type=True),\n",
       " Array([ 0.17560976, -0.61463415], dtype=float64, weak_type=True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_implicit_fwd = jacrev(implicit_func, argnums=(1, 2), has_aux=False)\n",
    "jac_x, jac_para = derivative_implicit_fwd(func, x_guess, para)\n",
    "jac_x, jac_para.alpha, jac_para.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.8 ms ± 628 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_implicit_naive(func, x_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 ms ± 144 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_implicit_fwd(func, x_guess, para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With default initial guess\n",
    "This implementation is due to the fact that the initial is generated inside canoak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.custom_jvp, nondiff_argnums=(0,))\n",
    "def implicit_func_nox(func, para):\n",
    "    x_guess = jnp.array([1.5, 3.5])\n",
    "    func = eqx.tree_at(lambda t: t.para, func, para)\n",
    "    x_solution = fixed_point(func, x_guess)\n",
    "    return x_solution\n",
    "\n",
    "\n",
    "@implicit_func_nox.defjvp\n",
    "def implicit_func_nox_jvp(func, primals, tangents):\n",
    "    def func_para(func, x, para):\n",
    "        func = eqx.tree_at(lambda t: t.para, func, para)\n",
    "        return func(x)\n",
    "\n",
    "    args = primals\n",
    "    tangents_args = tangents\n",
    "\n",
    "    para = args[0]\n",
    "    x_solution = implicit_func_nox(func, para)\n",
    "\n",
    "    _, u = jvp(partial(func_para, func, x_solution), args, tangents_args, has_aux=False)\n",
    "    Jacobian_JAX = jacfwd(func, argnums=0, has_aux=False)\n",
    "    J = Jacobian_JAX(x_solution)\n",
    "    I = jnp.eye(J.shape[0])\n",
    "    tangent_out = jnp.linalg.solve(I - J, u)\n",
    "    return (\n",
    "        x_solution,\n",
    "        tangent_out,\n",
    "    )  # you don't need to add None, see the discussion here (https://github.com/google/jax/discussions/16871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 0.1804878 , -0.13170732], dtype=float64, weak_type=True),\n",
       " Array([ 0.17560976, -0.61463415], dtype=float64, weak_type=True))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_implicit_nox_fwd = jacrev(implicit_func_nox, argnums=(1), has_aux=False)\n",
    "jac_para = derivative_implicit_nox_fwd(func, para)\n",
    "jac_para.alpha, jac_para.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (TODO) What if I only want to compute the gradient against one output?\n",
    "- Here, I have to create a function that outputs only one value. This function is essentially another callable pytree. However, it is also a eqx.Partial or jax.tree_util.Partial instance.\n",
    "So, the IFT gradient funciton needs to be revised accordingly.\n",
    "- Also, this time, I embedded the iteration inside the main function in a way similar to canoak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([2., 3.], dtype=float64), Array(2., dtype=float64))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Let's redefine the classes\n",
    "# class Para(eqx.Module):\n",
    "#     alpha: Float[Array, \"\"]\n",
    "#     beta: Float[Array, \"\"]\n",
    "\n",
    "# class Fc(eqx.Module):\n",
    "#     para: Para\n",
    "\n",
    "#     def __call__(self) -> Array:\n",
    "#         x_guess = jnp.array([1.5, 3.5])\n",
    "#         def func(x):\n",
    "#             F_0 = jnp.sqrt(self.para.alpha-x[0]*x[1])\n",
    "#             F_1 = jnp.sqrt((57-x[1])/(self.para.beta*F_0))\n",
    "#             return jnp.array([F_0, F_1])\n",
    "#         return fixed_point(func, x_guess)\n",
    "\n",
    "#     def output1(self) -> Array:\n",
    "#         return self()[0]\n",
    "\n",
    "# alpha_v = 10.\n",
    "# beta_v = 3.0\n",
    "\n",
    "# para = Para(alpha_v, beta_v)\n",
    "\n",
    "# func = Fc(para)\n",
    "\n",
    "# # func(jnp.array([1.5, 3.5])), func.output1(jnp.array([1.5, 3.5]))\n",
    "# func(), func.output1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(jax.custom_jvp, nondiff_argnums=(0,))\n",
    "# def implicit_func_nox_partial(func, para):\n",
    "#     func = eqx.tree_at(lambda t: t.args[0].para, func, para)\n",
    "#     return func()\n",
    "\n",
    "# @implicit_func_nox_partial.defjvp\n",
    "# def implicit_func_nox_partial_jvp(func, primals, tangents):\n",
    "#     def func_para(func, para):\n",
    "#         func = eqx.tree_at(lambda t: t.args[0].para, func, para)\n",
    "#         return func()\n",
    "#     args = primals\n",
    "#     tangents_args = tangents\n",
    "\n",
    "#     para = args[0]\n",
    "#     x_solution = implicit_func_nox_partial(func, para)\n",
    "#     print(x_solution)\n",
    "\n",
    "#     _, u = jvp(partial(func_para, func), args, tangents_args, has_aux=False)\n",
    "#     Jacobian_JAX = jacfwd(func, argnums=0, has_aux=False)\n",
    "#     J = Jacobian_JAX(x_solution)\n",
    "#     I = jnp.eye(J.shape[0])\n",
    "#     tangent_out = jnp.linalg.solve(I-J, u)\n",
    "#     return x_solution, tangent_out # you don't need to add None, see the discussion here (https://github.com/google/jax/discussions/16871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative_implicit_nox_partial_fwd = jacrev(implicit_func_nox_partial, argnums=(1), has_aux=False)\n",
    "# jac_para = derivative_implicit_nox_partial_fwd(func.output1, para)\n",
    "# jac_para.alpha,jac_para.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (TODO) What if the outputs are two pytrees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Para(eqx.Module):\n",
    "    alpha: Float[Array, \"\"]\n",
    "    beta: Float[Array, \"\"]\n",
    "\n",
    "\n",
    "class Var(eqx.Module):\n",
    "    value: Float[Array, \"\"]\n",
    "\n",
    "\n",
    "class Fc(eqx.Module):\n",
    "    para: Para\n",
    "\n",
    "    def __call__(self, x: Tuple[Var, Var]) -> Tuple[Var, Var]:\n",
    "        F_0 = jnp.sqrt(self.para.alpha - x[0].value * x[1].value)\n",
    "        F_1 = jnp.sqrt((57 - x[1].value) / (self.para.beta * F_0))\n",
    "        # return F_0, F_1\n",
    "        xnew = [Var(value=F_0), Var(value=F_1)]\n",
    "        return xnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Var(value=f64[]), Var(value=f64[])]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_v = 10.0\n",
    "beta_v = 3.0\n",
    "x_guess = [Var(1.5), Var(3.5)]\n",
    "para = Para(alpha_v, beta_v)\n",
    "func = Fc(para)\n",
    "func(x_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Var(value=f64[]), Var(value=f64[])]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fixed_point_pytree(f, x_guess):\n",
    "    def body_fun(x, i):\n",
    "        x_new = f(x)\n",
    "        return x_new, None\n",
    "\n",
    "    x_star, _ = jax.lax.scan(body_fun, x_guess, xs=None, length=1000)\n",
    "    return x_star\n",
    "\n",
    "\n",
    "fixed_point_pytree(func, x_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Var(value=[Var(value=f64[]), Var(value=f64[])]),\n",
       " Var(value=[Var(value=f64[]), Var(value=f64[])])]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jacobian_JAX_pytree = jacfwd(func, argnums=0, has_aux=False)\n",
    "Jacobian_JAX_pytree(x_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.custom_jvp, nondiff_argnums=(0,))\n",
    "def implicit_func(func, x_guess, para):\n",
    "    func = eqx.tree_at(lambda t: t.para, func, para)\n",
    "    x_solution = fixed_point(func, x_guess)\n",
    "    return x_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@implicit_func.defjvp\n",
    "def implicit_func_jvp(func, primals, tangents):\n",
    "    def func_para(func, x, para):\n",
    "        func = eqx.tree_at(lambda t: t.para, func, para)\n",
    "        return func(x)\n",
    "\n",
    "    x_guess, args = primals[0], primals[1:]\n",
    "    tangents_x, tangents_args = tangents[0], tangents[1:]\n",
    "\n",
    "    para = args[0]\n",
    "    x_solution = implicit_func(func, x_guess, para)\n",
    "\n",
    "    _, u = jvp(partial(func_para, func, x_solution), args, tangents_args, has_aux=False)\n",
    "    Jacobian_JAX = jacfwd(func, argnums=0, has_aux=False)\n",
    "    J = Jacobian_JAX(x_solution)\n",
    "    # J, u = jnp.array(J), jnp.array(u)\n",
    "    I = jnp.eye(J.shape[0])\n",
    "    tangent_out = jnp.linalg.solve(I - J, u)\n",
    "    return (\n",
    "        x_solution,\n",
    "        tangent_out,\n",
    "    )  # you don't need to add None, see the discussion here (https://github.com/google/jax/discussions/16871)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-watershed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
