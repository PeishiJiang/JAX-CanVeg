"""
Functions used for performing optimization using optax.

Author: Peishi Jiang
Date: 2023.08.22.
"""

import jax
import jax.tree_util as jtu

import optax
import equinox as eqx
import jax.numpy as jnp

from typing import Tuple, List
from jaxtyping import Array

from ...models import CanoakBase
from ...subjects import Met, BatchedMet


# Define the loss function
@eqx.filter_value_and_grad
def loss_func(
    diff_model: CanoakBase, static_model: CanoakBase, y: Array, met: Met, *args
):
    """Calculating the gradient with respect to diff_model.
       Note that diff_model and static_model has the same type and
       can be generated by using the filtering strategy. See an example here:
       https://docs.kidger.site/equinox/examples/frozen_layer/.

    Args:
        diff_model (CanoakBase): _description_
        static_model (CanoakBase): _description_
        y (Array): _description_
        met (Met): _description_

    Returns:
        _type_: _description_
    """
    model = eqx.combine(diff_model, static_model)
    pred_y = model(met, *args)
    return jnp.mean((y - pred_y) ** 2)


def perform_optimization(
    model: CanoakBase,
    filter_model_spec: CanoakBase,
    optim: optax._src.base.GradientTransformation,
    y: Array,
    met: Met,
    nsteps: int,
    *args,
) -> Tuple[CanoakBase, List]:
    """A wrapped function for performing optimization using optax.

    Args:
        model (CanoakBase): _description_
        filter_model_spec (CanoakBase): _description_
        optim (optax._src.base.GradientTransformation): _description_
        y (Array): _description_
        met (Met): _description_
        nsteps (int): _description_

    Returns:
        Tuple[CanoakBase, List]: _description_
    """

    @eqx.filter_jit
    def make_step(model, filter_model_spec, y, opt_state, met, *args):
        diff_model, static_model = eqx.partition(model, filter_model_spec)
        loss, grads = loss_func(diff_model, static_model, y, met, *args)
        updates, opt_state = optim.update(grads, opt_state)
        model = eqx.apply_updates(model, updates)
        return model, opt_state, loss, grads

    loss_set = []
    # opt_state = optim.init(model)
    opt_state = optim.init(eqx.filter(model, eqx.is_array))
    for i in range(nsteps):
        model, opt_state, loss, grads = make_step(
            model, filter_model_spec, y, opt_state, met, *args
        )
        loss_set.append(loss)
        print(f"The loss of step {i}: {loss}")

    return model, loss_set


def perform_optimization_batch(
    model: CanoakBase,
    filter_model_spec: CanoakBase,
    optim: optax._src.base.GradientTransformation,
    batched_y: Array,
    batched_met: BatchedMet,
    nsteps: int,
    *args,
) -> Tuple[CanoakBase, List]:
    """A wrapped function for performing optimization in batch using optax.

    Args:
        model (CanoakBase): _description_
        filter_model_spec (CanoakBase): _description_
        optim (optax._src.base.GradientTransformation): _description_
        y (Array): _description_
        met (Met): _description_
        nsteps (int): _description_

    Returns:
        Tuple[CanoakBase, List]: _description_
    """

    @eqx.filter_jit
    def make_step(model, filter_model_spec, batched_y, opt_state, batched_met, *args):
        print("Compiling make_step ...")
        diff_model, static_model = eqx.partition(model, filter_model_spec)
        # loss, grads = loss_func(diff_model, static_model, y, met)
        def loss_func_batch(c, batch):
            met, y = batch
            loss, grads = loss_func(diff_model, static_model, y, met, *args)
            return c, [loss, grads]

        _, results = jax.lax.scan(loss_func_batch, None, xs=[batched_met, batched_y])
        loss = results[0].mean()
        # grads = results[1].mean()
        grads = jtu.tree_map(lambda x: x.mean(), results[1])

        updates, opt_state = optim.update(grads, opt_state)
        model = eqx.apply_updates(model, updates)
        return model, opt_state, loss, grads

    loss_set = []
    # opt_state = optim.init(model)
    opt_state = optim.init(eqx.filter(model, eqx.is_array))
    for i in range(nsteps):
        check_arg(model, "model")
        check_arg(filter_model_spec, "filter_model_spec")
        check_arg(batched_y, "batched_y")
        check_arg(opt_state, "opt_state")
        check_arg(batched_met, "batched_met")
        model, opt_state, loss, grads = make_step(
            model, filter_model_spec, batched_y, opt_state, batched_met, *args
        )
        loss_set.append(loss)
        print(f"The loss of step {i}: {loss}")

    return model, loss_set


@eqx.filter_jit
def check_arg(arg, name):
    print(f"Argument {name} is triggering a compile.")
